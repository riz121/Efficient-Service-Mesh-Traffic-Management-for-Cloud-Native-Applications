"""
mcg_config_generator.py

Implements:
Input: G(V, E) Microservice Communication Graph
Input: P Traffic steering decisions (thresholds & policies)
Input: T Monitoring window (seconds)
Input: DB  Metrics database adapter

Steps:
1. For each vertex: collect CPU/memory usage -> propose resource config
2. For each edge: collect lambda(t), r(t) -> weighted averages -> compare to thresholds -> apply policies
3. Build configuration YAML dict (routing, connection pool, retry/timeout)
4. Deploy via Configuration Updater adapter
"""

from dataclasses import dataclass, field
from typing import Dict, Tuple, List, Any
import time
import yaml


# ---------------------------
# Data structures
# ---------------------------
@dataclass
class Vertex:
    name: str
    replicas: int = 1
    request_limit: int = 0
    pending_requests: int = 0
    # Suggested resources (to be filled)
    suggested_cpu: float = None  # in cores
    suggested_memory_mb: int = None


@dataclass
class Edge:
    src: str
    dst: str
    weight: Tuple[float, float] = field(default_factory=lambda: (0.0, 0.0))  # (lambda_avg, r_avg)
    policies: Dict[str, Any] = field(default_factory=dict)  # e.g. circuit_breaker, retry, timeout


@dataclass
class Graph:
    vertices: Dict[str, Vertex] = field(default_factory=dict)
    edges: Dict[Tuple[str, str], Edge] = field(default_factory=dict)


# ---------------------------
# Pluggable adapters (to implement)
# ---------------------------
class MetricsDBAdapter:
    """
    Abstract adapter interface for metrics DB.
    Implement concrete methods to query real metrics (Prometheus/Influx/etc.)
    """

    def get_cpu_memory_timeseries(self, service_name: str, time_window: int) -> Dict[str, List[float]]:
        """
        Return dict: {"cpu": [core_values...], "memory_mb": [mb_values...]}
        where lists are samples across the monitoring window.
        """
        raise NotImplementedError

    def get_edge_metrics_timeseries(self, src: str, dst: str, time_window: int) -> Dict[str, List[float]]:
        """
        Return dict: {"lambda": [requests_per_sec...], "response_time_ms": [ms...]}
        """
        raise NotImplementedError


class ConfigUpdaterAdapter:
    """
    Adapter to apply generated configuration.
    Implement deployment logic: call istioctl, kubectl, or a REST API to push config.
    """
    def deploy_yaml(self, yaml_str: str) -> bool:
        """
        Deploy YAML string to the control plane/config store.
        Return True on success.
        """
        raise NotImplementedError


# ---------------------------
# Utility helpers
# ---------------------------
def moving_average(values: List[float]) -> float:
    if not values:
        return 0.0
    # Compute average digit-by-digit (explicit to avoid mental arithmetic mistakes)
    total = 0.0
    for v in values:
        total += float(v)
    return total / len(values)


# ---------------------------
# Core algorithm
# ---------------------------
class MCGConfigGenerator:
    def __init__(self, graph: Graph, policies: Dict, time_window: int,
                 db_adapter: MetricsDBAdapter, deployer: ConfigUpdaterAdapter):
        self.graph = graph
        self.policies = policies
        self.time_window = time_window
        self.db = db_adapter
        self.deployer = deployer

    def analyze_vertices(self):
        """
        For each vertex: collect CPU/memory usage and set suggested resource configs.
        """
        for name, v in self.graph.vertices.items():
            samples = self.db.get_cpu_memory_timeseries(name, self.time_window)
            cpu_samples = samples.get("cpu", [])
            mem_samples = samples.get("memory_mb", [])

            cpu_avg = moving_average(cpu_samples)
            mem_avg = moving_average(mem_samples)

            # Simple heuristic: suggest 1.5x average as request to allow headroom,
            # but min values to avoid 0. Note: tune for your environment.
            v.suggested_cpu = max(0.1, round(cpu_avg * 1.5, 3))  # cores
            v.suggested_memory_mb = max(32, int(mem_avg * 1.5))

            # Debug/info
            print(f"[Vertex] {name}: cpu_avg={cpu_avg:.3f} cores -> suggested_cpu={v.suggested_cpu} cores, "
                  f"mem_avg={mem_avg:.1f}MB -> suggested_mem={v.suggested_memory_mb}MB")

    def analyze_edges(self):
        """
        For each edge: collect lambda(t) and r(t), compute weighted averages,
        compare to thresholds and apply policies (circuit breaking, retry).
        """
        for (src, dst), edge in list(self.graph.edges.items()):
            timeseries = self.db.get_edge_metrics_timeseries(src, dst, self.time_window)
            lambda_samples = timeseries.get("lambda", [])
            r_samples = timeseries.get("response_time_ms", [])

            lambda_avg = moving_average(lambda_samples)
            r_avg = moving_average(r_samples)

            edge.weight = (lambda_avg, r_avg)
            edge.policies = {}

            # Check thresholds from P (policies)
            # Example P config structure:
            # P = {
            #   "latency_ms_threshold": 200,
            #   "error_rate_threshold": 0.05,
            #   "lambda_threshold": 1000,
            #   "circuit_breaker": {...},
            #   "retry_policy": {...}
            # }
            if lambda_avg > self.policies.get("lambda_threshold", float("inf")):
                # High traffic: maybe increase connection pool or apply rate limiting
                edge.policies["scale_conn_pool"] = True

            if r_avg > self.policies.get("latency_ms_threshold", float("inf")):
                # Latency exceeded: apply retry/backoff or circuit breaking depending on severity
                cb_conf = self.policies.get("circuit_breaker", {"max_consecutive_errors": 5, "interval_ms": 10000})
                retry_conf = self.policies.get("retry_policy", {"retries": 2, "per_try_timeout_ms": 200})
                edge.policies["circuit_breaker"] = cb_conf
                edge.policies["retry"] = retry_conf

            # Example: add an latency-based timeout if nearing threshold
            if r_avg > 0.8 * self.policies.get("latency_ms_threshold", float("inf")):
                edge.policies["timeout_ms"] = int(1.2 * r_avg)

            print(f"[Edge] {src} -> {dst}: λ̄={lambda_avg:.2f}, r̄={r_avg:.2f}ms, policies={edge.policies}")

    def generate_config_yaml(self) -> str:
        """
        Builds a YAML string representing routing, connection pools, retry/timeouts, and resource suggestions.
        YAML structure is illustrative and should be adapted to your control-plane schema (e.g., Istio VirtualService/DestinationRule).
        """
        config = {"resources": {}, "routing": [], "connection_pools": {}, "retries_timeouts": {}}

        # Vertex resource suggestions
        for name, v in self.graph.vertices.items():
            config["resources"][name] = {
                "cpu_cores": v.suggested_cpu,
                "memory_mb": v.suggested_memory_mb,
                "replicas": v.replicas
            }

        # Edge-level routing and policies
        for (src, dst), edge in self.graph.edges.items():
            lambda_avg, r_avg = edge.weight
            # Routing example: percent split (1: default 100), could be informed by traffic steering P
            routing_entry = {
                "from": src,
                "to": dst,
                "traffic_percent": self.policies.get("default_split_percent", 100),
                "observed_lambda": lambda_avg,
                "observed_latency_ms": r_avg
            }
            config["routing"].append(routing_entry)

            # Connection pool suggestion
            if edge.policies.get("scale_conn_pool"):
                config["connection_pools"][f"{src}->{dst}"] = {"max_connections": int(max(10, lambda_avg / 10))}

            # Retry / timeout / circuit-breaking rules
            rule = {}
            if "retry" in edge.policies:
                rule["retry"] = edge.policies["retry"]
            if "timeout_ms" in edge.policies:
                rule["timeout_ms"] = edge.policies["timeout_ms"]
            if "circuit_breaker" in edge.policies:
                rule["circuit_breaker"] = edge.policies["circuit_breaker"]
            if rule:
                config["retries_timeouts"][f"{src}->{dst}"] = rule

        yaml_str = yaml.safe_dump(config, sort_keys=False)
        print("[Config YAML]\n", yaml_str)
        return yaml_str

    def deploy(self, yaml_str: str) -> bool:
        """
        Use deployer adapter to apply configuration. Returns boolean status.
        """
        success = self.deployer.deploy_yaml(yaml_str)
        print(f"[Deploy] Deployment {'succeeded' if success else 'failed'}.")
        return success

    def run(self):
        """
        Full pipeline.
        """
        print("Starting MCG configuration generation pipeline...")
        self.analyze_vertices()
        self.analyze_edges()
        yaml_str = self.generate_config_yaml()
        ok = self.deploy(yaml_str)
        return ok


# ---------------------------
# Mock adapters for testing
# ---------------------------
import random


class MockMetricsDB(MetricsDBAdapter):
    def get_cpu_memory_timeseries(self, service_name: str, time_window: int) -> Dict[str, List[float]]:
        # Return synthetic samples: cpu in cores, mem in MB
        n_samples = max(3, time_window // 10)
        return {
            "cpu": [round(random.uniform(0.05, 0.5) * (1 + hash(service_name) % 3), 3) for _ in range(n_samples)],
            "memory_mb": [random.uniform(50, 300) for _ in range(n_samples)]
        }

    def get_edge_metrics_timeseries(self, src: str, dst: str, time_window: int) -> Dict[str, List[float]]:
        n_samples = max(3, time_window // 10)
        # lambda: requests/sec, response_time_ms
        return {
            "lambda": [random.uniform(10, 1000) for _ in range(n_samples)],
            "response_time_ms": [random.uniform(5, 500) for _ in range(n_samples)]
        }


class MockDeployer(ConfigUpdaterAdapter):
    def deploy_yaml(self, yaml_str: str) -> bool:
        # For demo just print a message and return True
        print("[MockDeployer] Pretending to deploy YAML to control plane...")
        # Optionally validate YAML structure
        try:
            parsed = yaml.safe_load(yaml_str)
            # Basic sanity check:
            if "resources" not in parsed:
                print("[MockDeployer] YAML invalid (no resources).")
                return False
        except Exception as e:
            print("[MockDeployer] YAML parse error:", e)
            return False
        time.sleep(0.5)
        return True


# ---------------------------
# Example usage
# ---------------------------
if __name__ == "__main__":
    # Build a sample graph
    g = Graph()
    for svc in ["productpage", "reviews", "ratings", "details"]:
        g.vertices[svc] = Vertex(name=svc, replicas=2, request_limit=200, pending_requests=0)

    g.edges[("productpage", "reviews")] = Edge(src="productpage", dst="reviews")
    g.edges[("reviews", "ratings")] = Edge(src="reviews", dst="ratings")
    g.edges[("productpage", "details")] = Edge(src="productpage", dst="details")

    # Example traffic steering decisions / thresholds
    P = {
        "latency_ms_threshold": 200,
        "lambda_threshold": 800,
        "circuit_breaker": {"max_consecutive_errors": 6, "interval_ms": 10000, "base_sleep_ms": 200},
        "retry_policy": {"retries": 2, "per_try_timeout_ms": 150},
        "default_split_percent": 100
    }

    time_window = 60  # seconds

    db = MockMetricsDB()
    deployer = MockDeployer()

    generator = MCGConfigGenerator(graph=g, policies=P, time_window=time_window, db_adapter=db, deployer=deployer)
    ok = generator.run()
    print("Pipeline finished:", ok)
